时间复杂度是在编码时,对代码的执行时间有一个大概的分析,无关执行平台和数据规模的影响.

时间复杂度一般用大O表示法来表示,可以省略掉系数、低阶、常量

简单来说一段代码没有任何循环,开头到结尾所有语句只会执行一次,记作O(n)

如果循环了n次(n=元素数量) 那么记作O(n)

>  几种分析思路:
>
> 不关心所有常量执行次数
>
> 只关心循环执行次数最多的一段代码
>
> 加法法则:总复杂度等于量级最大的那段代码的复杂度
>
> 乘法法则:嵌套代码的复杂度等于嵌套内外代码的复杂度的乘积 假设 T1(n) = O(n)，T2(n) = O(n2)，则 T1(n) * T2(n) = O(n3)

> 几种常见的时间复杂度:
>
> * **O(1)** 一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。
> * **O(logn),O(nlogn)** 我们知道，对数之间是可以互相转换的，log3n 就等于 log32 * log2n，所以 O(log3n) = O(C * log2n)，其中 C=log32 是一个常量。基于我们前面的一个理论：在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了。
> * **O(m=n),O(m*n)** 由两个数据的规模来决定



> 最好,最坏时间复杂度:
>
> * 最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度.最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。比如在一个数组中查找一个元素,如果在第一位就找到,那么就是O(1),如果在最后一位找到,那么就是O(n)
>
> 平均时间复杂度:
>
> * 根据算法计算时可能发生的时间复杂度,根据概率权重求出最有可能的情况,
>
> 均摊时间复杂度:
>
> * 均摊时间复杂度就是一种特殊的平均时间复杂度,比如list的add方法,在容量足够的时候add就直接加到末尾,时间为O(1),容量满的时候新建一个数组把旧的元素copy过去再添加,时间为O(n),执行次数总是N次添加1次扩容,所以这个算法均摊下来就为O(1)



